model_id: elyza/Llama-3-ELYZA-JP-8B
runtime: transformers
attn_implementation: flash_attention_2
load_in_4bit: true
compute_dtype: bfloat16
temperature: 0.3
top_p: 0.9
repetition_penalty: 1.05
max_new_tokens: 512
