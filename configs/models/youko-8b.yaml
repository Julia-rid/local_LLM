model_id: rinna/llama-3-youko-8b-instruct
runtime: transformers
attn_implementation: flash_attention_2
load_in_4bit: true
compute_dtype: bfloat16
temperature: 0.35
top_p: 0.9
repetition_penalty: 1.1
max_new_tokens: 512
